{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b110a5a-3f75-4423-86b0-396cef16071d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langgraph\n",
    "!pip install langchain-openai\n",
    "!pip install langchain_community\n",
    "!pip install langchain tavily-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a67d2d3-e2ac-4c7b-bd6c-4c3910beb2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter your OpenAI API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5295e3ae-caee-4208-8891-57969dc3e56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"TAVILY_API_KEY\" not in os.environ:\n",
    "    os.environ[\"TAVILY_API_KEY=your_tavily_api_key\"] = getpass(\"Enter your Tavily API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16922561-389c-4ecd-8c95-f3e6023bbaf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langgraph\n",
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict, Annotated\n",
    "import operator\n",
    "from langchain_core.messages import AnyMessage, SystemMessage, HumanMessage, ToolMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.tools.tavily_search import TavilySearchResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a0da1f-2861-4a7c-be2b-1c951f41765c",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' We will use the `TavilySearchResults` tool to perform real-time web searches.'''\n",
    "\n",
    "tool = TavilySearchResults(max_results=4) #increased number of results\n",
    "print(type(tool))\n",
    "print(tool.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841cdab3-8483-44be-98e6-8abe8361fa02",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Defining an agent state \n",
    "    - To track full conversation history\n",
    "    - Automatically merge messages across steps (e.g., LLM → tool → LLM)\n",
    "    - Used by every node in the LangGraph to get/update the state'''\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], operator.add]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a9b9b1-836c-4152-96ec-ec916d95023c",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Defining an AI agent that:\n",
    "\n",
    "- Uses a language model to generate thoughts and actions.\n",
    "\n",
    "- Executes actions (i.e. tools) if requested by the model.\n",
    "\n",
    "- Loops back with the results (observations) to inform the final answer.\n",
    "\n",
    "\n",
    "exists_action(self, state)\n",
    "- Checks if the latest LLM message contains tool calls.\n",
    "- Used to decide whether to run action or end the graph\n",
    "\n",
    "call_openai(self, state)\n",
    "- Sends all accumulated messages to the LLM, including an optional system prompt.\n",
    "- Returns the assistant’s next message.\n",
    "\n",
    "take_action(self, state)\n",
    "- Executes tools requested by the LLM:\n",
    "- Iterates over tool_calls\n",
    "- Looks up each tool by name\n",
    "- Invokes it with the provided args-\n",
    "- Returns the results as ToolMessages\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "class Agent:\n",
    "\n",
    "    def __init__(self, model, tools, system=\"\"):\n",
    "        self.system = system\n",
    "        graph = StateGraph(AgentState)\n",
    "        graph.add_node(\"llm\", self.call_openai)\n",
    "        graph.add_node(\"action\", self.take_action)\n",
    "        graph.add_conditional_edges(\n",
    "            \"llm\",\n",
    "            self.exists_action,\n",
    "            {True: \"action\", False: END}\n",
    "        )\n",
    "        graph.add_edge(\"action\", \"llm\")\n",
    "        graph.set_entry_point(\"llm\")\n",
    "        self.graph = graph.compile()\n",
    "        self.tools = {t.name: t for t in tools}\n",
    "        self.model = model.bind_tools(tools)\n",
    "\n",
    "    def exists_action(self, state: AgentState):\n",
    "        result = state['messages'][-1]\n",
    "        return len(result.tool_calls) > 0\n",
    "\n",
    "    def call_openai(self, state: AgentState):\n",
    "        messages = state['messages']\n",
    "        if self.system:\n",
    "            messages = [SystemMessage(content=self.system)] + messages\n",
    "        message = self.model.invoke(messages)\n",
    "        return {'messages': [message]}\n",
    "\n",
    "    def take_action(self, state: AgentState):\n",
    "        tool_calls = state['messages'][-1].tool_calls\n",
    "        results = []\n",
    "        for t in tool_calls:\n",
    "            print(f\"Calling: {t}\")\n",
    "            if not t['name'] in self.tools:      # check for bad tool name from LLM\n",
    "                print(\"\\n ....bad tool name....\")\n",
    "                result = \"bad tool name, retry\"  # instruct LLM to retry if bad\n",
    "            else:\n",
    "                result = self.tools[t['name']].invoke(t['args'])\n",
    "            results.append(ToolMessage(tool_call_id=t['id'], name=t['name'], content=str(result)))\n",
    "        print(\"Back to the model!\")\n",
    "        return {'messages': results}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf127e1-6f78-4f99-9d1d-cb043ab5cb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"You are a smart research assistant. Use the search engine to look up information. \\\n",
    "You are allowed to make multiple calls (either together or in sequence). \\\n",
    "Only look up information when you are sure of what you want. \\\n",
    "If you need to look up some information before asking a follow up question, you are allowed to do that!\n",
    "\"\"\"\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\")  #reduce inference cost\n",
    "abot = Agent(model, [tool], system=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbb3cb2-544f-4814-89c0-a1e47fad8633",
   "metadata": {},
   "outputs": [],
   "source": [
    "conda install -c conda-forge pygraphviz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4878adf-2009-49a0-a3b3-a137b636d149",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "Image(abot.graph.get_graph().draw_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e3781d-f09a-4206-9773-4dbfa5528d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [HumanMessage(content=\"What is the weather in sf?\")]\n",
    "result = abot.graph.invoke({\"messages\": messages})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e4b189-7731-48f3-9bf6-a2ffb135f776",
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983cc5c2-6ed9-4ecc-88d1-002fb26bd757",
   "metadata": {},
   "outputs": [],
   "source": [
    "result['messages'][-1].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba262a2-2bee-4d99-9819-13c519055d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Who won the super bowl in 2024? In what state is the winning team headquarters located? \\\n",
    "What is the GDP of that state? Answer each question.\" \n",
    "messages = [HumanMessage(content=query)]\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o\")  # requires more advanced model\n",
    "abot = Agent(model, [tool], system=prompt)\n",
    "result = abot.graph.invoke({\"messages\": messages})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8395be-055d-4a80-a300-a402daef8ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result['messages'][-1].content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
